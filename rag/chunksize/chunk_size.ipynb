{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 确定文本切割的最优策略\n",
    "\n",
    "在使用基于检索的生成模型（RAG）处理长文本数据时，合理的文本切割策略是提高模型性能和效率的关键。\n",
    "\n",
    "文本切割策略主要依赖于两个参数：`chunksize`（块大小）和`overlap`（重叠）。正确配置这些参数可以显著影响模型的输出质量和处理速度。\n",
    "\n",
    "* chunk_size 基于模型的限制(embedding  , LLM )\n",
    "* 不同Text splitter 的优劣，如何选取\n",
    "* 可视化文本切分的效果，供大家切分文本初步参考\n",
    "\n",
    "## 基于模型选取**chunk_size** <a id=\"**chunk_size**\"></a>\n",
    "\n",
    "* 首先是**embedding model**， 向量嵌入模型有**Max Tokens** 的限制，设置的**chunk size**不可以超过模型支持的最大长度，否则将丢失语义。\n",
    " \n",
    "![Alt text](<chunksize VS embe max tokens.png>)\n",
    "\n",
    "不同的**embedding model** 支持的 **Max Tokens**都有不同，具体可参考[model 排行](https://huggingface.co/spaces/mteb/leaderboard)\n",
    "\n",
    "* 其次是**LLM model** , 大语言模型有**Max sequence length**的限制，处理知识增强的时候，**prompt**中召回的文本不可以超出最大长度。\n",
    "\n",
    "![Alt text](<chunksize VS LLM.png>)\n",
    "\n",
    "需要根据不同的LLM支持的最大token长度，选取合适的参数\n",
    "\n",
    "**不同的文本切分策略**\n",
    "* **1: [CharacterTextSplitter](#CharacterTextSplitter)** - 这是最简单的方法。它默认基于字符（默认为\"\"）来分割，并且通过字符的数量来衡量块的长度。\n",
    "* **2：[RecursiveCharacterTextSplitter](#RecursiveCharacterTextSplitter)** - 基于字符列表拆分文本。\n",
    "* **3: [Document Specific Splitting](#DocumentSpecific)** - 基于不同的文件类型使用不同的切分 (PDF, Python, Markdown)\n",
    "* **4: [Semantic Splitting](#SemanticChunking)** - 基于滑动窗口的语义切分\n",
    "\n",
    "那我们就开始实际看一下不同的textsplitter切分效果如何？"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "! pip install langchain"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "text = \"大家好，我是果粒奶优有果粒，欢迎关注我，让我们一起探索AI\""
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. CharacterTextSplitter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\",\n",
    "    chunk_size=5,\n",
    "    chunk_overlap=1,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "text_splitter.split_text(text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "切分原理\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#创建chunks维护切分的文本块\n",
    "chunks = []\n",
    "chunk_size = 5 \n",
    "chunk_overlap = 1\n",
    "\n",
    "i = 0\n",
    "while i < len(text):\n",
    "    # 如果这不是第一块，就回溯chunk_overlap个字符以创建重叠\n",
    "    if i > 0:\n",
    "        start = max(i - chunk_overlap, 0)\n",
    "    else:\n",
    "        start = i\n",
    "    # 确定这个块的结束位置\n",
    "    end = min(start + chunk_size, len(text))  \n",
    "    # 提取块并添加到列表\n",
    "    chunk = text[start:end]\n",
    "    chunks.append(chunk)\n",
    "    # 更新下一块的开始位置\n",
    "    i = end\n",
    "print(chunks)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2 RecursiveCharacterTextSplitter\n",
    "**RecursiveCharacterTextSplitter**文本分割工具的设计目的是为了在处理文本时，能够在不损失语义关联性的前提下，将文本有效分割成更小的单元。通过先尝试分割段落，如果段落仍然过大，再尝试分割成句子，依此类推，直至分割成单词。这种分割方法尽量保留文本的原有结构和意义，使得处理后的文本单元在语义上保持连贯性。\n",
    "\n",
    "* \"\\n\\n\" - 段落\n",
    "* \"\\n\" - 换行\n",
    "* \" \" - 空格\n",
    "* \"\" - 字符"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "text = '''  \n",
    "为什么文本切割在RAG中很重要？RAG（Retrieval-Augmented Generation）是一种将检索机制集成到生成式语言模型中的技术，目的是通过从大量文档或知识库中检索相关信息来增强模型的生成能力。这种方法特别适用于需要广泛背景知识的任务，如问答、文章撰写或详细解释特定主题。在RAG架构中，文本切割（即将长文本分割成较短片段的过程）非常重要，原因如下：\n",
    "\n",
    "1. **提高检索效率：** 对于大规模的文档库，直接在整个库上执行检索任务既不切实际也不高效。通过将长文本切割成较短的片段，可以使检索过程更加高效，因为短文本片段更容易被比较和索引。这样可以加快检索速度，提高整体性能。\n",
    "\n",
    "2. **提升结果相关性：** 当查询特定信息时，与查询最相关的内容往往只占据文档中的一小部分。通过文本切割，可以更精确地匹配查询和文档片段之间的相关性，从而提高检索到的信息的准确性和相关性。这对于生成高质量、相关性强的回答尤为重要。\n",
    "\n",
    "3. **内存和处理限制：** 当代的语言模型，尽管强大，但处理长文本时仍受到内存和计算资源的限制。将长文本分割成较短的片段可以帮助减轻这些限制，因为模型可以分别处理这些较短的文本片段，而不是一次性处理整个长文档。\n",
    "\n",
    "4. **提高生成质量：** 在RAG框架中，检索到的文本片段将直接影响生成模块的输出。通过确保检索到高质量和高相关性的文本片段，可以提高最终生成内容的质量和准确性。\n",
    "\n",
    "5. **适应性和灵活性：** 文本切割允许模型根据需要处理不同长度的文本，增加了模型处理各种数据源的能力。这种灵活性对于处理多样化的查询和多种格式的文档特别重要。\n",
    "\n",
    "总之，文本切割在RAG中非常重要，因为它直接影响到检索效率、结果的相关性、系统的处理能力，以及最终生成内容的质量和准确性。通过优化文本切割策略，可以显著提升RAG系统的整体性能和用户满意度。'''"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=1,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    "    separators = [\"\\n\\n\", \"\\n\", \" \" , \"\"]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chunk_doc = text_splitter.create_documents([text])"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chunk_doc"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "! pip install sentence-transformers\n",
    "! pip install matplotlib\n",
    "! pip install transformers\n",
    "! pip install pandas"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt \n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "Embedding_name = 'BAAI/bge-large-zh-v1.5'"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "SentenceTransformer(Embedding_name).max_seq_length\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "len(chunk_doc[1].page_content)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chunk_doc[1].page_content"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(Embedding_name)\n",
    "len(tokenizer.encode(chunk_doc[1].page_content))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### token数量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_chunk(chunk_doc , Embedding_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Embedding_name)\n",
    "    length = [len(tokenizer.encode(doc.page_content))\n",
    "                  for doc in chunk_doc ]\n",
    "    fig = pd.Series(length).hist()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_chunk(chunk_doc , Embedding_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3 其他结构的文本切割\n",
    "\n",
    "* **python**    - RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON)\n",
    "* **json**      - RecursiveJsonSplitter\n",
    "* **Markdown**  - MarkdownTextSplitter\n",
    "* **Html**      - HTMLHeaderTextSplitter\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4 Semantic Chunking <a id=\"SemanticChunking\"></a>\n",
    "\n",
    "为什么我们在处理文本时通常会使用固定的分块大小，而不考虑实际内容的语义意义。是不是可以基于文本的语义实现一种更好的方法来处理文本分块，即并非固定参数(**chunksize**)，而是基于语义自行动态确定参数。\n",
    "\n",
    "我们可以通过**embedding**技术进行动态规划\n",
    "\n",
    "**Embedding**将文本转化为高维空间中的向量的技术，这些向量能够反映出文本的语义内容。通过文本嵌入技术，可以捕捉到文本的深层次语义信息。当比较两段文本的嵌入向量时，可以根据它们在高维空间中的距离或者角度，来推断这两段文本在语义上的相似度或者差异。利用相似度，将语义上相似的文本自动分组在一起，形成聚类，这有助于更好地理解和组织大量的文本数据。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open('dream.txt') as file:\n",
    "    essay = file.read()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 我们需要将文本进行拆分，拆分成多个单句，可以按照标点符号进行切分\n",
    "\n",
    "split_char = ['.', '?', '!']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Splitting the essay on '.', '?', and '!'\n",
    "single_sentences_list = re.split(r'(?<=[.?!])\\s+', essay)\n",
    "print (f\"{len(single_sentences_list)} senteneces were found\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "single_sentences_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们需要为单个句子拼接更多的句子，但是 `list` 添加比较困难。因此将其转换为字典列表（`List[dict]`）\n",
    "\n",
    "{ 'sentence' : XXX  , 'index' : 0}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sentences = [{'sentence': x, 'index' : i} for i, x in enumerate(single_sentences_list)]\n",
    "sentences[:3]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def combine_sentences(sentences, buffer_size=1):\n",
    "    # \n",
    "    \n",
    "    combined_sentences = [\n",
    "        ' '.join(sentences[j]['sentence'] for j in range(max(i - buffer_size, 0), min(i + buffer_size + 1, len(sentences))))\n",
    "        for i in range(len(sentences))\n",
    "    ]   \n",
    "    # 更新原始字典列表，添加组合后的句子\n",
    "    for i, combined_sentence in enumerate(combined_sentences):\n",
    "        sentences[i]['combined_sentence'] = combined_sentence\n",
    "\n",
    "    return sentences\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sentences = combine_sentences(sentences)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sentences[:6]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下来使用**embedding model**对**sentences** 进行编码"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass()\n",
    "\n",
    "oaiembeds = OpenAIEmbeddings()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "embeddings = oaiembeds.embed_documents([x['combined_sentence'] for x in sentences])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "将embedding添加到sentence中"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(sentences):\n",
    "    sentence['combined_sentence_embedding'] = embeddings[i]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sentences[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下来需要根据余弦相似度进行切分"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate the cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cosine_similarity(sentences[0]['combined_sentence_embedding'], sentences[1]['combined_sentence_embedding'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def calculate_cosine_distances(sentences):\n",
    "    distances = []\n",
    "    for i in range(len(sentences) - 1):\n",
    "        embedding_current = sentences[i]['combined_sentence_embedding']\n",
    "        embedding_next = sentences[i + 1]['combined_sentence_embedding']\n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity(embedding_current, embedding_next)\n",
    "        # Convert to cosine distance\n",
    "        distance = 1 - similarity\n",
    "        distances.append(distance)\n",
    "        # Store distance in the dictionary\n",
    "        sentences[i]['distance_to_next'] = distance\n",
    "    return distances, sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "distances, sentences = calculate_cosine_distances(sentences)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sentences[-2]['distance_to_next']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(distances);"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "有很多方法可以基于这些距离来划分论文，但我打算将任何超过距离95百分位数的距离视为一个分割点。这是我们需要配置的唯一参数。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "plt.plot(distances)\n",
    "\n",
    "y_upper_bound = 0.15\n",
    "plt.ylim(0, y_upper_bound)\n",
    "plt.xlim(0, len(distances))\n",
    "\n",
    "\n",
    "# We need to get the distance threshold that we'll consider an outlier\n",
    "# We'll use numpy .percentile() for this\n",
    "breakpoint_percentile_threshold = 95\n",
    "breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold) # If you want more chunks, lower the percentile cutoff\n",
    "plt.axhline(y=breakpoint_distance_threshold, color='r', linestyle='-')\n",
    "num_distances_above_theshold = len([x for x in distances if x > breakpoint_distance_threshold]) # The amount of distances above your threshold\n",
    "plt.text(x=(len(distances)*.01), y=y_upper_bound/50, s=f\"{num_distances_above_theshold + 1} Chunks\")\n",
    "\n",
    "# Then we'll get the index of the distances that are above the threshold. This will tell us where we should split our text\n",
    "indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold] # The indices of those breakpoints on your list\n",
    "\n",
    "# Start of the shading and text\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "\n",
    "for i, breakpoint_index in enumerate(indices_above_thresh):\n",
    "    start_index = 0 if i == 0 else indices_above_thresh[i - 1]\n",
    "    end_index = breakpoint_index if i <= len(indices_above_thresh) - 1 else len(distances)\n",
    "\n",
    "    plt.axvspan(start_index, end_index, facecolor=colors[i % len(colors)], alpha=0.25)\n",
    "    plt.text(x=np.average([start_index, end_index]),\n",
    "            y=breakpoint_distance_threshold + (y_upper_bound)/ 20,\n",
    "            s=f\"Chunk #{i}\", horizontalalignment='center',\n",
    "            rotation='vertical')\n",
    "# # Additional step to shade from the last breakpoint to the end of the dataset\n",
    "if indices_above_thresh:\n",
    "    last_breakpoint = indices_above_thresh[-1]\n",
    "    if last_breakpoint < len(distances):\n",
    "        plt.axvspan(last_breakpoint, len(distances), facecolor=colors[len(indices_above_thresh) % len(colors)], alpha=0.25)\n",
    "        plt.text(x=np.average([last_breakpoint, len(distances)]),\n",
    "                 y=breakpoint_distance_threshold + (y_upper_bound)/ 20,\n",
    "                 s=f\"Chunk #{i+1}\",\n",
    "                 rotation='vertical')\n",
    "plt.title(\"Essay Chunks Based On Embedding Breakpoints\")\n",
    "plt.xlabel(\"Index of sentences in essay (Sentence Position)\")\n",
    "plt.ylabel(\"Cosine distance between sequential sentences\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Initialize the start index\n",
    "start_index = 0\n",
    "\n",
    "# Create a list to hold the grouped sentences\n",
    "chunks = []\n",
    "\n",
    "# Iterate through the breakpoints to slice the sentences\n",
    "for index in indices_above_thresh:\n",
    "    # The end index is the current breakpoint\n",
    "    end_index = index\n",
    "\n",
    "    # Slice the sentence_dicts from the current start index to the end index\n",
    "    group = sentences[start_index:end_index + 1]\n",
    "    combined_text = ' '.join([d['sentence'] for d in group])\n",
    "    chunks.append(combined_text)\n",
    "    \n",
    "    # Update the start index for the next group\n",
    "    start_index = index + 1\n",
    "\n",
    "# The last group, if any sentences remain\n",
    "if start_index < len(sentences):\n",
    "    combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n",
    "    chunks.append(combined_text)\n",
    "\n",
    "# grouped_sentences now contains the chunked sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks[:2]):\n",
    "    buffer = 200\n",
    "    print (f\"Chunk #{i}\")\n",
    "    print (chunk[:buffer].strip())\n",
    "    print (\"...\")\n",
    "    print (chunk[-buffer:].strip())\n",
    "    print (\"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
