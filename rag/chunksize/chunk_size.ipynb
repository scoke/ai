{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 确定文本切割的最优策略\n",
    "在使用基于检索的生成模型（RAG）处理长文本数据时，合理的文本切割策略是提高模型性能和效率的关键。\n",
    "\n",
    "文本切割策略主要依赖于两个参数：chunksize（块大小）和overlap（重叠）。正确配置这些参数可以显著影响模型的输出质量和处理速度。\n",
    "   - chunk_size 基于模型的限制(embedding , LLM )\n",
    "   - 不同Text splitter 的优劣，如何选取\n",
    "   - 可视化文本切分的效果，供大家切分文本初步参考\n",
    "      \n",
    "## 基于模型选取chunk_size \n",
    "   - 首先是embedding model， 向量嵌入模型有Max Tokens 的限制，设置的chunk size不可以超过模型支持的最大长度，否则将丢失语义。\n",
    "\n",
    "\n",
    "不同的embedding model 支持的 Max Tokens都有不同，具体可参考model 排行\n",
    "其次是LLM model , 大语言模型有Max sequence length的限制，处理知识增强的时候，prompt中召回的文本不可以超出最大长度。\n",
    "![Alt text]( )\n",
    "需要根据不同的LLM支持的最大token长度，选取合适的参数\n",
    "不同的文本切分策略\n",
    "1: CharacterTextSplitter - 这是最简单的方法。它默认基于字符（默认为\"\"）来分割，并且通过字符的数量来衡量块的长度。\n",
    "2：RecursiveCharacterTextSplitter - 基于字符列表拆分文本。\n",
    "3: Document Specific Splitting - 基于不同的文件类型使用不同的切分 (PDF, Python, Markdown)\n",
    "4: Semantic Splitting - 基于滑动窗口的语义切分\n",
    "那我们就开始实际看一下不同的textsplitter切分效果如何？\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "text = \"大家好，我是果粒奶优有果粒，欢迎关注我，让我们一起探索AI\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T14:08:22.575410Z",
     "start_time": "2024-04-24T14:08:22.572187Z"
    }
   },
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. CharacterTextSplitter\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_text_splitters'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_text_splitters\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CharacterTextSplitter\n\u001B[1;32m      2\u001B[0m text_splitter \u001B[38;5;241m=\u001B[39m CharacterTextSplitter(\n\u001B[1;32m      3\u001B[0m     separator\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      4\u001B[0m     chunk_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      7\u001B[0m     is_separator_regex\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m      8\u001B[0m )\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'langchain_text_splitters'"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\",\n",
    "    chunk_size=5,\n",
    "    chunk_overlap=1,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T14:08:22.674850Z",
     "start_time": "2024-04-24T14:08:22.662494Z"
    }
   },
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "text_splitter.split_text(text)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
